/**
 * AUTO-GENERATED FILE - DO NOT EDIT
 * Generated by: bun scripts/embed-assets.ts
 * Generated at: 2025-12-11T13:28:22.296Z
 *
 * Embedded rule templates for use in compiled binaries.
 */

export const EMBEDDED_RULES: Record<string, string> = {
  "00-overview": `# Long-Task Harness

This project uses the **agent-foreman** harness for feature-driven development with AI agents.

## Core Files

| File | Purpose |
|------|---------|
| \`ai/feature_list.json\` | Feature backlog with status tracking |
| \`ai/progress.log\` | Session handoff audit log |
| \`ai/init.sh\` | Bootstrap script (install/dev/check) |

## Feature Status Values

- \`failing\` - Not yet implemented or incomplete
- \`passing\` - Acceptance criteria met
- \`blocked\` - External dependency blocking
- \`needs_review\` - Potentially affected by recent changes
- \`failed\` - Implementation attempted but verification failed
- \`deprecated\` - No longer needed

## Feature Selection Priority

When running \`agent-foreman next\`, features are selected in this order:
1. **Status first**: \`needs_review\` > \`failing\` (other statuses excluded)
2. **Then priority number**: Lower number = higher priority (1 is highest)

Example: A feature with \`priority: 1\` runs before \`priority: 10\`
`,
  "01-workflow": `# Workflow for Each Session

## Mode Detection

**If a feature_id is provided** (e.g., \`agent-foreman next auth.login\`):
- Work on that specific feature only
- Complete it and stop

**If no feature_id** (e.g., \`agent-foreman next\`):
- Auto-select highest priority pending feature
- Process features in priority order

---

## Single Feature Mode

When feature_id is provided:

\`\`\`bash
# STEP 1: Get the specified feature
agent-foreman next <feature_id>

# STEP 2: Implement feature
# (satisfy ALL acceptance criteria shown)

# STEP 3: Verify implementation (required)
agent-foreman check <feature_id>

# STEP 4: Complete feature (skips re-verification since we just checked)
agent-foreman done <feature_id>
\`\`\`

---

## All Features Mode

When no feature_id:

\`\`\`bash
# STEP 1: Check status
agent-foreman status

# STEP 2: Get next feature
agent-foreman next

# STEP 3: Implement feature
# (satisfy ALL acceptance criteria shown)

# STEP 4: Verify implementation (required)
agent-foreman check <feature_id>

# STEP 5: Complete feature (skips re-verification since we just checked)
agent-foreman done <feature_id>

# STEP 6: Handle result
# - Verification passed? → Continue to STEP 1
# - Verification failed? → Mark as failed, continue to STEP 1
# - All features processed? → STOP, show summary
\`\`\`

---

## Rules (MUST Follow)

| Rule | Action |
|------|--------|
| No skipping | Always: status → next → implement → check → done |
| One at a time | Complete current before next |
| No editing criteria | Implement exactly as specified |
| Never kill processes | Let commands finish naturally |

---

## On Verification Failure

When \`agent-foreman done\` reports verification failure:

1. **DO NOT STOP** - Continue to the next feature
2. Mark the failed feature as \`failed\`:
   - Edit \`ai/feature_list.json\`
   - Change \`"status": "failing"\` to \`"status": "failed"\`
   - Add to notes: \`"Verification failed: [reason from output]"\`
3. Log the failure in \`ai/progress.log\`:
   \`\`\`
   YYYY-MM-DDTHH:MM:SSZ VERIFY feature=<id> verdict=fail summary="Marked as failed"
   \`\`\`
4. Continue to the next feature immediately

**CRITICAL: NEVER stop due to verification failure - always mark as \`failed\` and continue!**

---

## Exit Conditions

| Condition | Action |
|-----------|--------|
| All features processed | ✅ STOP - Show summary |
| Single feature completed | ✅ STOP - Feature done |
| User interrupts | ⏹️ STOP - Clean state |

---

## Loop Completion

When all features have been processed:

1. Run \`agent-foreman status\` to show final summary
2. Report counts:
   - ✓ X features passing
   - ⚡ Y features failed (need investigation)
   - ⚠ Z features needs_review (dependency changes)
   - ✗ W features still failing (not attempted)
3. List features that failed verification with their failure reasons

---

## Priority Order (Auto-Selected)

1. \`needs_review\` → highest priority
2. \`failing\` → next priority
3. Lower \`priority\` number within same status
`,
  "02-rules": `# Rules

1. **One feature per session** - Complete or pause cleanly before switching
2. **Don't modify acceptance criteria** - Only change \`status\` and \`notes\`
3. **Update status promptly** - Mark features passing when criteria met
4. **Leave clean state** - No broken code between sessions
5. **Use single-line log format** - One line per entry, not verbose Markdown
6. **Never kill running processes** - Let \`agent-foreman\` commands complete naturally, even if they appear slow or timed out. They may be doing important work (verification, git commits, survey regeneration). Just wait for completion.
7. **Use CI=true for tests** - Always set \`CI=true\` environment variable when running any test commands (e.g., \`CI=true npm test\`, \`CI=true pnpm test\`, \`CI=true vitest\`) to ensure non-interactive mode and consistent behavior.
`,
  "03-commands": `# Commands

\`\`\`bash
# View project status
agent-foreman status

# Work on next priority feature
agent-foreman next

# Work on specific feature
agent-foreman next <feature_id>

# Verify feature implementation (without marking complete)
agent-foreman check <feature_id>

# Mark feature as done (skips verification by default, use after check)
agent-foreman done <feature_id>

# Mark feature as done (with verification, for manual use)
agent-foreman done <feature_id> --no-skip-check

# Full mode - run all tests (slower, for final verification)
agent-foreman done <feature_id> --full --no-skip-check

# Skip E2E tests (faster iterations)
agent-foreman done <feature_id> --skip-e2e

# Skip auto-commit (manual commit)
agent-foreman done <feature_id> --no-commit

# Disable loop mode (no continuation reminder)
agent-foreman done <feature_id> --no-loop

# Analyze impact of changes
agent-foreman impact <feature_id>

# Scan project verification capabilities
agent-foreman scan

# Bootstrap/development/testing
./ai/init.sh bootstrap
./ai/init.sh dev
./ai/init.sh check
./ai/init.sh check --quick  # Selective testing mode
\`\`\`
`,
  "04-feature-schema": `# Feature JSON Schema

**IMPORTANT**: When adding or modifying features in \`ai/feature_list.json\`, use this exact schema.

**Note**: \`priority\` uses lower number = higher priority (1 is highest, 10 is lower).

\`\`\`json
{
  "features": [
    {
      "id": "module.feature.action",
      "description": "Human-readable description of the feature",
      "module": "parent-module-name",
      "priority": 1,
      "status": "failing",
      "acceptance": [
        "First acceptance criterion",
        "Second acceptance criterion"
      ],
      "dependsOn": ["other.feature.id"],
      "supersedes": [],
      "tags": ["optional-tag"],
      "version": 1,
      "origin": "manual",
      "notes": "",
      "testRequirements": {
        "unit": { "required": false, "pattern": "tests/module/**/*.test.ts" }
      }
    }
  ],
  "metadata": {
    "projectGoal": "Project goal description",
    "createdAt": "2024-01-01T00:00:00.000Z",
    "updatedAt": "2024-01-01T00:00:00.000Z",
    "version": "1.0.0"
  }
}
\`\`\`

**Required fields**: \`id\`, \`description\`, \`module\`, \`priority\`, \`status\`, \`acceptance\`, \`version\`, \`origin\`

**Auto-generated fields**: \`testRequirements\` (auto-generated during init with pattern \`tests/{module}/**/*.test.*\`)

**Optional fields**: \`testRequirements\` (can be overridden), \`e2eTags\` (Playwright tags for E2E filtering)

## Feature ID Convention

Feature IDs use dot notation: \`module.submodule.action\`

Examples:
- \`auth.login\`
- \`chat.message.edit\`
- \`api.users.create\`

## Acceptance Criteria Format

Write criteria as testable statements:
- "User can submit the form and see a success message"
- "API returns 201 status with created resource"
- "Error message displays when validation fails"

## testRequirements Structure

\`\`\`json
"testRequirements": {
  "unit": {
    "required": false,
    "pattern": "tests/auth/**/*.test.ts",
    "cases": ["should login", "should logout"]
  },
  "e2e": {
    "required": false,
    "pattern": "e2e/auth/**/*.spec.ts",
    "tags": ["@auth"],
    "scenarios": ["user can login"]
  }
}
\`\`\`

- \`required: true\` - Feature cannot complete without matching test files (TDD enforcement)
- \`pattern\` - Glob pattern for selective test execution in quick mode
- \`cases\`/\`scenarios\` - Expected test names (optional, for documentation)

**Status values**: \`failing\` | \`passing\` | \`blocked\` | \`needs_review\` | \`failed\` | \`deprecated\`

**Origin values**: \`init-auto\` | \`init-from-routes\` | \`init-from-tests\` | \`manual\` | \`replan\`

`,
  "05-tdd": `# TDD Mode Configuration

The project's TDD enforcement is controlled by \`metadata.tddMode\` in \`ai/feature_list.json\`:

| Mode | Effect |
|------|--------|
| \`strict\` | Tests REQUIRED - check/done fail without tests |
| \`recommended\` (default) | Tests suggested but not enforced |
| \`disabled\` | No TDD guidance |

## Strict Mode Behavior

When \`tddMode: "strict"\`:
- \`agent-foreman check\` blocks if test files missing
- \`agent-foreman done\` blocks if test files missing
- All features auto-migrate to \`testRequirements.unit.required: true\`
- TDD workflow enforced: RED - GREEN - REFACTOR

## User Control via Natural Language

| User Says | Action |
|-----------|--------|
| "enable strict TDD" / "require tests" | Set \`tddMode: "strict"\` |
| "disable strict TDD" / "optional tests" | Set \`tddMode: "recommended"\` |
| "turn off TDD" | Set \`tddMode: "disabled"\` |

To change mode manually, edit \`ai/feature_list.json\`:
\`\`\`json
{
  "metadata": {
    "tddMode": "strict"
  }
}
\`\`\`

## TDD Workflow

Run \`agent-foreman next\` to see TDD guidance:
- Suggested test files for the current feature
- Acceptance criteria - test case mapping
- Test skeleton preview

Follow the **RED - GREEN - REFACTOR** cycle:
1. **RED**: View acceptance criteria (they are your failing tests)
2. **GREEN**: Write minimum code to satisfy criteria
3. **REFACTOR**: Clean up under test protection
`,
  "06-progress-log": `# Progress Log Format

Append entries to \`ai/progress.log\` using this **single-line format only**:

\`\`\`
2025-01-15T10:30:00Z STEP feature=auth.login status=passing summary="Implemented login flow"
2025-01-15T11:00:00Z CHANGE feature=auth.login action=refactor reason="Improved error handling"
2025-01-15T12:00:00Z REPLAN summary="Splitting auth into submodules" note="Original scope too large"
\`\`\`

**Log types**: \`INIT\` | \`STEP\` | \`CHANGE\` | \`REPLAN\` | \`VERIFY\`

**IMPORTANT**: Do NOT write verbose Markdown session notes. Keep each entry as a single line.
`
};

export const EMBEDDED_RULE_NAMES = Object.keys(EMBEDDED_RULES);
